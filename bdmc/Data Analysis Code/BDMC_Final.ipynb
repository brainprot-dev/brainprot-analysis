{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QvT73RFYeGg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlist = pd.read_excel(\"Disease_List_2.xlsx\")  # Change this file with the disease information\n",
        "NCBI_Database = \"NCBI_Human_Gene_Database_V1.csv\" # Upload this file in the local device\n",
        "ndf = pd.read_csv(NCBI_Database) "
      ],
      "metadata": {
        "id": "RBzmCkZHYmjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctd_markerdatabase = pd.DataFrame()"
      ],
      "metadata": {
        "id": "OfMwIYRTYo2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(dlist)):\n",
        "    # Extract the values of MESH ID, disease name, OMIM ID, DisGeNET Disease ID, Harmonizome Query Term\n",
        "    # from the dlist and assign them to corresponding variables\n",
        "    i = i+22\n",
        "    MESH_ID = dlist[\"MESH ID\"][i]\n",
        "    Disease_Name = dlist[\"diseaseName\"][i]\n",
        "    OMIM_ID = dlist[\"OMIM\"][i]\n",
        "    Disgenet_Disease_ID = dlist[\"DisGeNETDiseaseId\"][i]\n",
        "    harmonizome_queryterm = dlist[\"HarmonizomeQueryTerm\"][i]\n",
        "\n",
        "    # Replace the spaces in the Harmonizome query term with '+' and the apostrophes with '%27'\n",
        "    # and convert the string to lowercase\n",
        "    harmonizome_decease_name = harmonizome_queryterm.replace(' ', '+')\n",
        "    harmonizome_decease_name = harmonizome_decease_name.replace(\"'\", \"%27\")\n",
        "    harmonizome_decease_name = harmonizome_decease_name.lower()\n",
        "\n",
        "    # Create BIONDA and Pubpular filenames by replacing the spaces in the disease name with '_'\n",
        "    BIONDA_filename = Disease_Name + \" BIONDA\"\n",
        "    Pubpular_filename = Disease_Name + \" Pubpular\"\n",
        "\n",
        "    # Create a master folder path and create the folder if it does not already exist\n",
        "    master_folder_path = f\"./Master Folder 2/{Disease_Name}_{MESH_ID}\"\n",
        "    if not os.path.exists(master_folder_path):\n",
        "        os.makedirs(master_folder_path)\n",
        "\n",
        "    # Construct the CTD URL using MESH ID and disease name, and create a CTD filename\n",
        "    \n",
        "    ctd_url = f\"https://ctdbase.org/detail.go;jsessionid=385F14EFFD796AF9E5182714E9B94C73;jsessionid=35B21D1728528E4FF50BA934E3598789?acc=MESH%3A{MESH_ID}&view=gene&6578706f7274=1&type=disease&d-1332398-e=1\"\n",
        "    ctd_filename = f\"CTD_{Disease_Name}_{MESH_ID}_Only_reading.csv\"\n",
        "\n",
        "    if os.path.isfile(ctd_filename):\n",
        "        print(f\"File '{ctd_filename}' already exists in the directory. Skipping download.\")\n",
        "    else:\n",
        "        print(f\"File '{ctd_filename}' does not exist in the directory. Downloading...\")\n",
        "        # Download the CTD data from the URL and write it to a file\n",
        "        response = requests.get(ctd_url)\n",
        "        if response.status_code == 200:\n",
        "            with open(ctd_filename, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Successfully downloaded CTD data for {Disease_Name}\")\n",
        "        else:\n",
        "            # If download is not successful, save an empty file and print a message\n",
        "            print(f\"Download failed for CTD data for {Disease_Name}. Saving an empty file.\")\n",
        "            with open(ctd_filename, \"w\") as f:\n",
        "                f.write(\"\")\n",
        "            \n",
        "    \n",
        "\n",
        "    # Read the CTD data from the file and drop the 'Inference Network' column\n",
        "    df = pd.read_csv(ctd_filename)\n",
        "\n",
        "    if not df.empty:\n",
        "        df.drop(['Inference Network'], axis=1, inplace=True)\n",
        "\n",
        "        # Replace missing values in the 'Inference Score' column with '#N/A'\n",
        "        df['Inference Score'].fillna(value=\"#N/A\", inplace=True)\n",
        "\n",
        "        # Merge the CTD and ndf dataframes on 'Gene ID' and 'NCBI GeneID' columns, respectively,\n",
        "        # to get the common rows and extract the rows that have the disease ID same as MESH ID\n",
        "        common_rows = pd.merge(df, ndf, left_on='Gene ID', right_on='NCBI GeneID', how='inner')\n",
        "        ncdf = common_rows[common_rows[\"Disease ID\"] == f\"MESH:{MESH_ID}\"]\n",
        "\n",
        "        # Create a new column 'CTD_Marker' by applying a lambda function that assigns 1 to rows\n",
        "        # that have 'therapeutic' or 'marker/mechanism' in the 'Direct Evidence' column, 0 to rows\n",
        "        # that have NaN values in that column, and None to rows that have other values\n",
        "        ncdf['CTD_Marker'] = ncdf['Direct Evidence'].apply(lambda x: 1 if x in ['therapeutic', 'marker/mechanism'] else 0 if pd.isna(x) else None)\n",
        "        # Checking if 'Inference Score' column in 'ncdf' dataframe is numeric\n",
        "        if not pd.api.types.is_numeric_dtype(ncdf['Inference Score']):\n",
        "            # Converting the column to numeric type with errors coerced to NaN\n",
        "            ncdf['Inference Score'] = pd.to_numeric(ncdf['Inference Score'], errors='coerce')\n",
        "\n",
        "        # Taking the log base 10 of the 'Inference Score' column and storing it in a new 'Mod_CTD' column\n",
        "        ncdf['Mod_CTD'] = np.log10(ncdf['Inference Score'])\n",
        "\n",
        "        # Scaling 'Mod_CTD' column using MinMaxScaler\n",
        "        scaler = MinMaxScaler()\n",
        "        ncdf[\"Mod_CTD\"] = scaler.fit_transform(ncdf[[\"Mod_CTD\"]])\n",
        "\n",
        "        # Creating a new dataframe with selected columns from 'ncdf'\n",
        "        ctd_download = ncdf[['Gene Symbol', 'Gene ID', 'Disease Name', 'Disease ID',\n",
        "                'Direct Evidence', 'Inference Score', 'Reference Count','Mod_CTD','CTD_Marker']]\n",
        "\n",
        "        # Creating a filepath to save the downloaded CTD data in a csv file\n",
        "        ctd_file_path = os.path.join(master_folder_path, f\"CTD_{Disease_Name}_{MESH_ID}.csv\")\n",
        "\n",
        "        # Saving the 'ctd_download' dataframe as a csv file in the specified filepath\n",
        "        ctd_download.to_csv(ctd_file_path, index=False)\n",
        "\n",
        "        # Printing a success message indicating that CTD data has been downloaded\n",
        "        print(f\"Successfully Downloaded CTD Data for {Disease_Name}\")\n",
        "    else:\n",
        "        empty_df = pd.DataFrame()\n",
        "        ctd_file_path = os.path.join(master_folder_path, f\"CTD_{Disease_Name}_{MESH_ID}.csv\")\n",
        "        empty_df.to_csv(ctd_file_path, index=False)\n",
        "        print(f\"The CTD data for {Disease_Name} is empty. An empty CSV file has been saved.\")\n",
        "\n",
        "\n",
        "    # Creating a URL using OMIM_ID and scraping data from the website\n",
        "    Edager_URL = f\"http://edgar.biocomp.unibo.it/gene_disease_db/disease_static/{OMIM_ID}_static.html\"\n",
        "    try:\n",
        "        # Sending a GET request to the URL and parsing the HTML content using BeautifulSoup\n",
        "        response = requests.get(Edager_URL)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Finding the HTML table with id 'main_table'\n",
        "        gene_table = soup.find('table', {'id': 'main_table'})\n",
        "\n",
        "        # Extracting all table rows except the first one as it contains header information\n",
        "        edager_gene_rows = gene_table.find_all('tr')[1:]\n",
        "\n",
        "        # Extracting gene names from each row and storing them in a list 'edager_gene_names'\n",
        "        edager_gene_names = []\n",
        "        for row in edager_gene_rows:\n",
        "            gene_name = row.find('td').text\n",
        "            edager_gene_names.append(gene_name)\n",
        "\n",
        "    # Skipping the code if any URL error occurs\n",
        "    except (requests.exceptions.RequestException, AttributeError):\n",
        "        pass\n",
        "\n",
        "    harmonizome_URL = f\"https://maayanlab.cloud/Harmonizome/api/1.0/gene_set/{harmonizome_decease_name}/DISEASES+Text-mining+Gene-Disease+Assocation+Evidence+Scores\"\n",
        "\n",
        "    try:\n",
        "        response_API = requests.get(harmonizome_URL)\n",
        "        data = response_API.text\n",
        "        parse_json = json.loads(data)\n",
        "\n",
        "        Hz=[]\n",
        "        for ele in parse_json['associations']:\n",
        "            temp={}\n",
        "            temp['Har_Gene']=ele['gene']['symbol']\n",
        "            temp['standardizedValue']=ele[\"standardizedValue\"]\n",
        "            Hz.append(temp)\n",
        "\n",
        "        df = pd.DataFrame(Hz)\n",
        "\n",
        "    except:\n",
        "        # If there's an error, create an empty dataframe\n",
        "        df = pd.DataFrame()\n",
        "\n",
        "    \n",
        "    if not df.empty:\n",
        "        # copy the data\n",
        "        df_sklearn = df.copy()\n",
        "        \n",
        "        # apply normalization techniques\n",
        "        column = 'standardizedValue'\n",
        "        df_sklearn[\"Mod_Standard_Value\"] = MinMaxScaler().fit_transform(np.array(df_sklearn[column]).reshape(-1,1))\n",
        "        \n",
        "        # view normalized data  \n",
        "        harmonizome_file_path = os.path.join(master_folder_path, f\"Harmonizome_{Disease_Name}_{MESH_ID}.csv\")\n",
        "        df_sklearn.to_csv(harmonizome_file_path, index=False)\n",
        "        print(f\"Successfully Downloaded Harmonizome Data for {Disease_Name}\")\n",
        "    else:\n",
        "        # If the dataframe is empty, create an empty dataframe and print a message\n",
        "        df_sklearn = pd.DataFrame()\n",
        "        harmonizome_file_path = os.path.join(master_folder_path, f\"Harmonizome_{Disease_Name}_{MESH_ID}.csv\")\n",
        "        df_sklearn.to_csv(harmonizome_file_path, index=False)\n",
        "        print(f\"Harmonizome Data for {Disease_Name} is empty. Saving as an empty dataframe.\")\n",
        "\n",
        "\n",
        "    the_data = {\"email\": \"200040114@iitb.ac.in\", \"password\": \"Godayadav28\"}\n",
        "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
        "    # Execute the post\n",
        "    response_API3 = requests.post('https://www.disgenet.org/api/auth/', data=the_data, headers=headers)\n",
        "    data3=response_API3.text\n",
        "    parse_json3 = json.loads(data3)\n",
        "    parse_json3\n",
        "    soo=requests.Session()\n",
        "\n",
        "    soo.headers.update({\"Authorization\": \"Bearer 3e807426707eabc1ad1988efc6085870b13fd97c\"})\n",
        "\n",
        "    Disgenet_URL = f\"https://www.disgenet.org/api/gda/disease/{Disgenet_Disease_ID}\"\n",
        "    response_API2 = soo.get(Disgenet_URL)\n",
        "\n",
        "    data2=response_API2.text\n",
        "    parse_json2 = json.loads(data2)\n",
        "    data_new=[]\n",
        "    #Gene_id”, \"Gene\",”Uniprot”, \"Score_gda\", \"DPI_g\", \"EI_gda\"\n",
        "\n",
        "    for ele in parse_json2:\n",
        "        temp={}\n",
        "        temp[\"geneid\"] = ele[\"geneid\"]\n",
        "        temp[\"gene_symbol\"] = ele[\"gene_symbol\"]\n",
        "        temp[\"uniprotid\"] = ele[\"uniprotid\"]\n",
        "        temp[\"score\"] = ele[\"score\"]\n",
        "        temp[\"gene_dpi\"] = ele[\"gene_dpi\"]\n",
        "        temp[\"ei\"] = ele[\"ei\"]\n",
        "        data_new.append(temp)\n",
        "    \n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(data_new)\n",
        "    df['ei'] = df['ei'].fillna(0)\n",
        "    df[\"score_norm\"] = df[[\"score\"]]\n",
        "    df[\"gene_dpi_norm\"] = df[[\"gene_dpi\"]]\n",
        "    df[\"ei_norm\"] = df[[\"ei\"]]\n",
        "\n",
        "    # Calculate the mean of the normalized columns\n",
        "    mean_norm = (df[\"score_norm\"] + df[\"gene_dpi_norm\"] + df[\"ei_norm\"]) / 3\n",
        "\n",
        "    # Add the normalized mean as a new column\n",
        "    df[\"DISGENET_Score\"] = mean_norm    \n",
        "    \n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    import numpy as np\n",
        "    \n",
        "    # copy the data\n",
        "    Disgenet = df.copy()\n",
        "    \n",
        "    column = 'DISGENET_Score'\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df[\"score_norm\"] = scaler.fit_transform(df[[\"score\"]])\n",
        "    df[\"gene_dpi_norm\"] = scaler.fit_transform(df[[\"gene_dpi\"]])\n",
        "    df[\"ei_norm\"] = scaler.fit_transform(df[[\"ei\"]])\n",
        "\n",
        "    MOD_mean_norm = (df[\"score_norm\"] + df[\"gene_dpi_norm\"] + df[\"ei_norm\"]) / 3\n",
        "\n",
        "\n",
        "    Disgenet[\"MOD_DISGENET_Score\"] = MOD_mean_norm\n",
        "    Disgenet[\"MOD_DISGENET_Score\"] = scaler.fit_transform(Disgenet[[\"MOD_DISGENET_Score\"]])\n",
        "    Disgenet_file_path = os.path.join(master_folder_path, f\"DISGenet_{Disease_Name}_{MESH_ID}.csv\")\n",
        "    Disgenet.to_csv(Disgenet_file_path, index=False)\n",
        "    print(f\"Successfully Downloaded DISGenet Data for {Disease_Name}\")\n",
        "\n",
        "    \n",
        "\n",
        "    # Download the file for each disease and upload it to the local device\n",
        "    Pubpular = pd.read_csv(f\"D:\\\\OneDrive - Indian Institute of Technology Bombay\\\\Desktop\\\\3rd Year 2nd Semester\\\\DH 307\\\\22nd March\\\\1.BIONDA and PubPular Files\\\\{Disease_Name}\\\\{Pubpular_filename}.csv\")\n",
        "\n",
        "    Pubpular_df = Pubpular[[\"WCD\"]] \n",
        "    Pubpular_arr = Pubpular_df.to_numpy()\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(Pubpular_arr)\n",
        "    Pubpular_array = scaler.transform(Pubpular_arr)\n",
        "\n",
        "    Pubpular_Norm_WCD = (1-Pubpular_array)\n",
        "\n",
        "    Pubpular_WCD = pd.DataFrame(Pubpular_array, columns = ['Norm_WCD'])\n",
        "    Pubpular_Final_WCD = pd.DataFrame(Pubpular_Norm_WCD, columns = ['Mod_Pubpular'])\n",
        "\n",
        "    Pubpular_Consolidate = pd.concat([Pubpular, Pubpular_WCD, Pubpular_Final_WCD], axis=1)\n",
        "    Pubpular_Final = Pubpular_Consolidate.fillna(0) # Fill NAN with 0\n",
        "\n",
        "    Pubpular_file_path = os.path.join(master_folder_path, f\"Pubpular_{Disease_Name}_{MESH_ID}.csv\")\n",
        "    Pubpular_Final.to_csv(Pubpular_file_path, index=False)\n",
        "    print(f\"Successfully Downloaded Pubpular Data for {Disease_Name}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Reads three CSV files using Pandas\n",
        "    ctd_df = pd.read_csv(f\"D:\\\\OneDrive - Indian Institute of Technology Bombay\\\\Desktop\\\\3rd Year 2nd Semester\\\\DH 307\\\\22nd March\\\\Master Folder 2\\\\{Disease_Name}_{MESH_ID}\\\\CTD_{Disease_Name}_{MESH_ID}.csv\") \n",
        "    \n",
        "    pubpular_df = pd.read_csv(f\"D:\\\\OneDrive - Indian Institute of Technology Bombay\\\\Desktop\\\\3rd Year 2nd Semester\\\\DH 307\\\\22nd March\\\\Master Folder 2\\\\{Disease_Name}_{MESH_ID}\\\\Pubpular_{Disease_Name}_{MESH_ID}.csv\") \n",
        "    try:\n",
        "        harmonizome_df = pd.read_csv(f\"D:\\\\OneDrive - Indian Institute of Technology Bombay\\\\Desktop\\\\3rd Year 2nd Semester\\\\DH 307\\\\22nd March\\\\Master Folder 2\\\\{Disease_Name}_{MESH_ID}\\\\Harmonizome_{Disease_Name}_{MESH_ID}.csv\")\n",
        "    \n",
        "    except: \n",
        "        harmonizome_df = pd.DataFrame(columns=['Har_Gene', 'standardizedValue', 'Mod_Standard_Value'])\n",
        "\n",
        "\n",
        "    disgenet_df = pd.read_csv(f\"D:\\\\OneDrive - Indian Institute of Technology Bombay\\\\Desktop\\\\3rd Year 2nd Semester\\\\DH 307\\\\22nd March\\\\Master Folder 2\\\\{Disease_Name}_{MESH_ID}\\\\DISGenet_{Disease_Name}_{MESH_ID}.csv\")\n",
        "\n",
        "    intermediate_df = pd.merge(ndf, ctd_df, how='outer', left_on='NCBI GeneID', right_on='Gene ID')\n",
        "\n",
        "    intermediate_df = pd.merge(intermediate_df, harmonizome_df, how='outer', left_on='Gene Name', right_on='Har_Gene')\n",
        "\n",
        "    intermediate_df = pd.merge(intermediate_df, pubpular_df, how='outer', left_on='Gene Name', right_on='GN')\n",
        "\n",
        "    intermediate_df = pd.merge(intermediate_df, disgenet_df, how='outer', left_on='NCBI GeneID', right_on='geneid')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Selects a subset of columns from the intermediate dataframe\n",
        "    intermediate_df = intermediate_df[['NCBI GeneID', 'Ensembl GeneIDs', 'OMIM IDs', 'SwissProt Accessions', 'Gene Name','DISGENET_Score', 'MOD_DISGENET_Score','standardizedValue','Mod_Standard_Value','Mod_Pubpular','WCD','Norm_WCD','Mod_CTD','CTD_Marker','Inference Score']]\n",
        "\n",
        "    # Selects a subset of columns to use for further processing\n",
        "    score_cols = ['MOD_DISGENET_Score', 'Mod_Standard_Value', 'Mod_Pubpular', 'Mod_CTD']\n",
        "\n",
        "    # Drops any rows from the intermediate dataframe where at least two of the score columns have NaN values\n",
        "    intermediate_df = intermediate_df.dropna(subset=score_cols, thresh=2)\n",
        "\n",
        "    # Adds a new column to the intermediate dataframe based on whether each gene name is in a list called \"edager_gene_names\"\n",
        "    intermediate_df[\"Edager_Marker\"] = intermediate_df[\"Gene Name\"].apply(lambda x: 1 if x in edager_gene_names else 0)\n",
        "\n",
        "\n",
        "    # check if the BIONDA file exists, if yes, extract required columns and create a new column \"BIONDA_Marker\" in the intermediate dataframe\n",
        "    # else, set \"BIONDA_Marker\" to \"#N/A\"\n",
        "    if os.path.exists(f\"D:\\\\OneDrive - Indian Institute of Technology Bombay\\\\Desktop\\\\3rd Year 2nd Semester\\\\DH 307\\\\22nd March\\\\1.BIONDA and PubPular Files\\\\{Disease_Name}\\\\{BIONDA_filename}.csv\"):\n",
        "        df = pd.read_csv(f\"D:\\\\OneDrive - Indian Institute of Technology Bombay\\\\Desktop\\\\3rd Year 2nd Semester\\\\DH 307\\\\22nd March\\\\1.BIONDA and PubPular Files\\\\{Disease_Name}\\\\{BIONDA_filename}.csv\")\n",
        "        bionda_columns = [\"Marker\", \"Gene\", \"To\"]\n",
        "        bionda_values = []\n",
        "        for col in bionda_columns:\n",
        "            # check if the column exists in the BIONDA file and add the column values to a list\n",
        "            if col in df.columns:\n",
        "                bionda_values += df[col].fillna('').tolist()\n",
        "        # create a new column \"BIONDA_Marker\" in the intermediate dataframe\n",
        "        # apply a lambda function to check if the \"Gene Name\" is in the bionda_values list, if yes, set the value to 1, else, set the value to 0\n",
        "        intermediate_df[\"BIONDA_Marker\"] = intermediate_df[\"Gene Name\"].apply(lambda x: 1 if x in bionda_values else 0)\n",
        "    else:\n",
        "        intermediate_df[\"BIONDA_Marker\"] = \"#N/A\"\n",
        "\n",
        "    # create a list of column names to apply the formulas on\n",
        "    columns_to_apply_formulas_on = ['MOD_DISGENET_Score', 'Mod_Standard_Value', 'Mod_Pubpular', 'Mod_CTD']\n",
        "\n",
        "    # calculate the new columns for all possible pairs of columns in the list using the formulas\n",
        "    for i in range(len(columns_to_apply_formulas_on)):\n",
        "        if columns_to_apply_formulas_on[i] not in intermediate_df.columns:\n",
        "            intermediate_df[columns_to_apply_formulas_on[i]] = 0\n",
        "            continue\n",
        "        for j in range(i+1, len(columns_to_apply_formulas_on)):\n",
        "            if columns_to_apply_formulas_on[j] not in intermediate_df.columns:\n",
        "                intermediate_df[columns_to_apply_formulas_on[j]] = 0\n",
        "                continue\n",
        "            # create a new column name by concatenating the two column names with \"top_\" in the beginning\n",
        "            colname = 'top_' + columns_to_apply_formulas_on[i].lower() + '_' + columns_to_apply_formulas_on[j].lower()\n",
        "            # apply the formula to calculate the value for each row in the new column\n",
        "            intermediate_df[colname] = ((intermediate_df[[columns_to_apply_formulas_on[i], columns_to_apply_formulas_on[j]]].mean(axis=1)) +\n",
        "                            (1 - (intermediate_df[[columns_to_apply_formulas_on[i], columns_to_apply_formulas_on[j]]].std(axis=1) /\n",
        "                                    intermediate_df[[columns_to_apply_formulas_on[i], columns_to_apply_formulas_on[j]]].mean(axis=1))) / 2)\n",
        "\n",
        "\n",
        "    \n",
        "    #dis2_df = pd.read_csv(\"/filepath\")\n",
        "    #dis2_names = list(dis2_df['Name'])\n",
        "\n",
        "    #intermediate_df['Dis_2_Marker'] = intermediate_df['Gene Name'].apply(lambda x: 1 if x in dis2_names else 0)\n",
        "\n",
        "    intermediate_df = intermediate_df.assign(Dis_2_Marker = 0)\n",
        "\n",
        "    \n",
        "    intermediate_df = intermediate_df.loc[:, ['SwissProt Accessions', 'Gene Name', 'MOD_DISGENET_Score', 'Mod_Standard_Value', 'Mod_Pubpular', 'Mod_CTD', 'top_mod_disgenet_score_mod_standard_value', 'top_mod_disgenet_score_mod_pubpular', 'top_mod_disgenet_score_mod_ctd', 'top_mod_standard_value_mod_pubpular', 'top_mod_standard_value_mod_ctd', 'top_mod_pubpular_mod_ctd', 'CTD_Marker', 'Edager_Marker', 'BIONDA_Marker', 'Dis_2_Marker']]\n",
        "\n",
        "\n",
        "    intermediate_df = intermediate_df.rename(columns={\n",
        "    'top_mod_disgenet_score_mod_standard_value': 'D_H',\n",
        "    'top_mod_disgenet_score_mod_pubpular': 'D_P',\n",
        "    'top_mod_disgenet_score_mod_ctd': 'D_C',\n",
        "    'top_mod_standard_value_mod_pubpular': 'H_P',\n",
        "    'top_mod_standard_value_mod_ctd': 'H_C',\n",
        "    'top_mod_pubpular_mod_ctd': 'P_C'\n",
        "    })\n",
        "\n",
        "    \n",
        "\n",
        "    intermediate_df.dropna(subset=['Gene Name', 'SwissProt Accessions'], how='all', inplace=True)\n",
        "\n",
        "    # create a master file path by joining the master folder path, disease name, and MESH ID\n",
        "    master_file_path = os.path.join(master_folder_path, f\"BDMC_Final_{Disease_Name}_{MESH_ID}.csv\")\n",
        "\n",
        "    # write the intermediate dataframe to a csv file with the master file path\n",
        "    # set the index to False to not include the index column in the output csv file\n",
        "    intermediate_df.to_csv(master_file_path, index=False)\n",
        "    # print a success message with the disease name\n",
        "    print(f\"Successfully Downloaded Master Data for {Disease_Name}\")"
      ],
      "metadata": {
        "id": "mM0bT0joYqhg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}